{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stds_out': array([0.00016265, 0.00023539, 0.00035341, 0.00048436]), 'stds_in': array([[4.84306531e-04, 4.56384200e-04, 3.55351348e-04, 2.81539874e-04,\n",
      "        1.35700998e-04, 9.78314171e-05, 5.57970035e-05, 5.52547789e-05,\n",
      "        4.14067768e+00, 2.55570006e+00, 4.93944022e-04, 4.24145375e-04,\n",
      "        3.82614729e-04, 3.17519557e-04, 2.34665233e-01, 5.10463055e-04,\n",
      "        4.43902196e-04, 6.77151468e+00, 6.38867514e+00, 9.04952108e+00,\n",
      "        1.01350294e+01, 1.20384457e+01, 1.26231835e+01, 1.34247109e+01,\n",
      "        1.35887536e+01]]), 'm_in': 42343, 'means_out': array([2.71655682e-07, 5.81106938e-07, 1.49434248e-06, 3.05222876e-06]), 'means_in': array([[ 3.06544175e-06,  3.09767064e-06,  3.10553429e-06,\n",
      "         3.12280327e-06,  3.59140599e-06,  4.24611616e-06,\n",
      "         8.19488016e-06,  1.26227555e-05, -6.34695040e+01,\n",
      "         7.50342914e+00,  5.12845357e-04, -5.07825753e-04,\n",
      "         5.63807669e-04, -5.67903882e-04,  5.66599774e-01,\n",
      "         4.84043880e-04, -4.77796802e-04,  1.27071275e+01,\n",
      "        -7.61652187e-03, -6.35417472e-02, -5.59344884e-02,\n",
      "         8.02694299e-02,  2.43427225e-01,  6.92004689e-01,\n",
      "        -2.05838736e-02]]), 'm_out': 42333}\n",
      "\ts 1 of 27. From 2016.04.25 03:00:00 to 2016.06.09 11:59:59\n",
      "init_date\n",
      "160425030000\n",
      "end_date\n",
      "160609115959\n",
      "group_name\n",
      "EURUSD/160425030000160609115959/\n",
      "features\n",
      "<HDF5 dataset \"features\": shape (42343, 25), type \"<f8\">\n",
      "<HDF5 dataset \"returns\": shape (42333, 4), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "# Train RNN\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from inputs import Data, load_separators, _build_bin_output\n",
    "from RNN import modelRNN\n",
    "from features import get_init_end_dates, get_group_name\n",
    "\n",
    "dateTest = ([                                                       '2018.03.09',\n",
    "                '2018.03.12','2018.03.13','2018.03.14','2018.03.15','2018.03.16',\n",
    "                '2018.03.19','2018.03.20','2018.03.21','2018.03.22','2018.03.23',\n",
    "                '2018.03.26','2018.03.27','2018.03.28','2018.03.29','2018.03.30',\n",
    "                '2018.04.02','2018.04.03','2018.04.04','2018.04.05','2018.04.06',\n",
    "                '2018.04.09','2018.04.10','2018.04.11','2018.04.12','2018.04.13',\n",
    "                '2018.04.16','2018.04.17','2018.04.18','2018.04.19','2018.04.20',\n",
    "                '2018.04.23','2018.04.24','2018.04.25','2018.04.26','2018.04.27',\n",
    "                '2018.04.30','2018.05.01','2018.05.02','2018.05.03','2018.05.04',\n",
    "                '2018.05.07','2018.05.08','2018.05.09','2018.05.10','2018.05.11',\n",
    "                '2018.05.14','2018.05.15','2018.05.16','2018.05.17','2018.05.18',\n",
    "                '2018.05.21','2018.05.22','2018.05.23','2018.05.24','2018.05.25',\n",
    "                '2018.05.28','2018.05.29','2018.05.30','2018.05.31','2018.06.01',\n",
    "                '2018.06.04','2018.06.05','2018.06.06','2018.06.07','2018.06.08',\n",
    "                '2018.06.11','2018.06.12','2018.06.13','2018.06.14','2018.06.15',\n",
    "                '2018.06.18','2018.06.19','2018.06.20','2018.06.21','2018.06.22',\n",
    "                '2018.06.25','2018.06.26','2018.06.27','2018.06.28','2018.06.29',\n",
    "                '2018.07.02','2018.07.03','2018.07.04','2018.07.05','2018.07.06',\n",
    "                '2018.07.09','2018.07.10','2018.07.11','2018.07.12','2018.07.13',\n",
    "                '2018.07.30','2018.07.31','2018.08.01','2018.08.02','2018.08.03',\n",
    "                '2018.08.06','2018.08.07','2018.08.08','2018.08.09','2018.08.10']+\n",
    "               ['2018.08.13','2018.08.14','2018.08.15','2018.08.16','2018.08.17',\n",
    "                '2018.08.20','2018.08.21','2018.08.22','2018.08.23','2018.08.24',\n",
    "                '2018.08.27','2018.08.28','2018.08.29','2018.08.30','2018.08.31',\n",
    "                '2018.09.03','2018.09.04','2018.09.05','2018.09.06','2018.09.07',\n",
    "                '2018.09.10','2018.09.11','2018.09.12','2018.09.13','2018.09.14',\n",
    "                '2018.09.17','2018.09.18','2018.09.19','2018.09.20','2018.09.21',\n",
    "                '2018.09.24','2018.09.25','2018.09.26','2018.09.27']+['2018.09.28',\n",
    "                '2018.10.01','2018.10.02','2018.10.03','2018.10.04','2018.10.05',\n",
    "                '2018.10.08','2018.10.09','2018.10.10','2018.10.11','2018.10.12',\n",
    "                '2018.10.15','2018.10.16','2018.10.17','2018.10.18','2018.10.19',\n",
    "                '2018.10.22','2018.10.23','2018.10.24','2018.10.25','2018.10.26',\n",
    "                '2018.10.29','2018.10.30','2018.10.31','2018.11.01','2018.11.02',\n",
    "                '2018.11.05','2018.11.06','2018.11.07','2018.11.08','2018.11.09'])\n",
    "\n",
    "data=Data(movingWindow=100,\n",
    "          nEventsPerStat=1000,\n",
    "          lB=1200, \n",
    "          dateTest=dateTest,\n",
    "          assets=[1,2,3,4,7,8,10,11,12,13,14,15,16,17,19,27,28,29,30,31,32],\n",
    "          channels=[0],\n",
    "          max_var=10,\n",
    "          feature_keys_manual=[],\n",
    "          feature_keys_tsfresh=[i for i in range(37,48)]+[49,50]+[i for i in range(52,68)],\n",
    "          var_feat_keys=[i for i in range(68,93)])\n",
    "\n",
    "model=modelRNN(data,\n",
    "               size_hidden_layer=100,\n",
    "               L=3,\n",
    "               size_output_layer=5,\n",
    "               keep_prob_dropout=1,\n",
    "               miniBatchSize=32,\n",
    "               outputGain=.6,\n",
    "               commonY=3,\n",
    "               lR0=0.0001,\n",
    "               num_epochs=1)\n",
    "\n",
    "if_build_IO = True\n",
    "from_stats_file = False\n",
    "calculate_roi = False\n",
    "IDweights = '0TESTVAR'\n",
    "hdf5_directory = 'D:/SDC/py/HDF5/'\n",
    "feats_var_directory = hdf5_directory+'fets_var/'\n",
    "IO_directory = '../RNN/IO/'\n",
    "filename_IO = IO_directory+'IO_'+IDweights+'.hdf5'\n",
    "separators_directory = hdf5_directory+'separators/'\n",
    "\n",
    "# if IO structures have to be built \n",
    "if if_build_IO:\n",
    "    # init dictionary containing IO structures\n",
    "    IO = {}\n",
    "    # open IO file for writting\n",
    "    f_IO = h5py.File(filename_IO,'w')\n",
    "    # init IO data sets\n",
    "    X = f_IO.create_dataset('X',\n",
    "                            (0, model.seq_len, model.nFeatures), \n",
    "                            maxshape=(None,model.seq_len, model.nFeatures), \n",
    "                            dtype=float)\n",
    "    Y = f_IO.create_dataset('Y',\n",
    "                            (0,model.seq_len,model.commonY+model.size_output_layer),\n",
    "                            maxshape=(None,model.seq_len,model.commonY+\n",
    "                            model.size_output_layer),\n",
    "                            dtype=float)\n",
    "    if calculate_roi:\n",
    "        D = f_IO.create_dataset('D', (0,model.seq_len,2),\n",
    "                                    maxshape=(None,model.seq_len,2),dtype='S19')\n",
    "        B = f_IO.create_dataset('B', (0,model.seq_len,2),\n",
    "                                    maxshape=(None,model.seq_len,2),dtype=float)\n",
    "        A = f_IO.create_dataset('A', (0,model.seq_len,2),\n",
    "                                    maxshape=(None,model.seq_len,2),dtype=float)\n",
    "        IO['D'] = D\n",
    "        IO['B'] = B\n",
    "        IO['A'] = A\n",
    "    # attributes to track asset-IO belonging\n",
    "    ass_IO_ass = np.zeros((len(data.assets))).astype(int)\n",
    "    # structure that tracks the number of samples per level\n",
    "    totalSampsPerLevel = np.zeros((model.size_output_layer))\n",
    "    # save IO structures in dictionary\n",
    "    IO['X'] = X\n",
    "    IO['Y'] = Y\n",
    "    IO['pointer'] = 0\n",
    "\n",
    "thisAsset = 'EURUSD'\n",
    "# load separators\n",
    "separators = load_separators(data, \n",
    "                             thisAsset, \n",
    "                             separators_directory, \n",
    "                             tOt='tr', \n",
    "                             from_txt=1)\n",
    "aloc = 2**20\n",
    "# index asset\n",
    "ass_idx = 0\n",
    "\n",
    "filename_features = (feats_var_directory+thisAsset+'_feats_var_mW'+str(data.movingWindow)+'_nE'+\n",
    "                            str(data.nEventsPerStat)+'.hdf5')\n",
    "file_features = h5py.File(filename_features,'r')\n",
    "filename_returns = (feats_var_directory+thisAsset+'_rets_var_mW'+str(data.movingWindow)+'_nE'+\n",
    "                            str(data.nEventsPerStat)+'.hdf5')\n",
    "file_returns = h5py.File(filename_returns,'r')\n",
    "filename_symbols = (feats_var_directory+thisAsset+'_symbols_mW'+str(data.movingWindow)+'_nE'+\n",
    "                            str(data.nEventsPerStat)+'.hdf5')\n",
    "file_symbols = h5py.File(filename_symbols,'r')\n",
    "filename_stats = (feats_var_directory+thisAsset+'_stats_mW'+str(data.movingWindow)+'_nE'+\n",
    "                                str(data.nEventsPerStat)+'.p')\n",
    "# init or load total stats\n",
    "if not from_stats_file:\n",
    "    stats = {}\n",
    "    # load stats in\n",
    "    stats[\"means_in\"] = file_features[thisAsset].attrs.get(\"means_in\")\n",
    "    stats[\"stds_in\"] = file_features[thisAsset].attrs.get(\"stds_in\")\n",
    "    stats[\"m_in\"] = file_features[thisAsset].attrs.get(\"m_in\")\n",
    "    # load stats out\n",
    "    stats[\"means_out\"] = file_returns[thisAsset].attrs.get(\"means_out\")\n",
    "    stats[\"stds_out\"] = file_returns[thisAsset].attrs.get(\"stds_out\")\n",
    "    stats[\"m_out\"] = file_returns[thisAsset].attrs.get(\"m_out\")\n",
    "elif from_stats_file:\n",
    "    stats = pickle.load( open( filename_stats, \"rb\" ))\n",
    "print(stats)\n",
    "\n",
    "nExS = data.nEventsPerStat\n",
    "mW = data.movingWindow\n",
    "\n",
    "s = 2\n",
    "\n",
    "print(\"\\ts {0:d} of {1:d}\".format(int(s/2),int(len(separators)/2-1))+\n",
    "                              \". From \"+separators.DateTime.iloc[s]+\" to \"+\n",
    "                              separators.DateTime.iloc[s+1])\n",
    "\n",
    "# number of events within this separator chunk\n",
    "nE = separators.index[s+1]-separators.index[s]+1\n",
    "# get first day after separator\n",
    "day_s = separators.DateTime.iloc[s][0:10]\n",
    "# check if number of events is not enough to build two features and one return\n",
    "if nE-nExS>=2*nExS:\n",
    "    if day_s not in data.dateTest and day_s<=data.dateTest[-1]:\n",
    "        # init and end dates\n",
    "        init_date, end_date = get_init_end_dates(separators, s)\n",
    "        print(\"init_date\")\n",
    "        print(init_date)\n",
    "        print(\"end_date\")\n",
    "        print(end_date)\n",
    "        # get group name\n",
    "        group_name = get_group_name(thisAsset, init_date, end_date)\n",
    "        print(\"group_name\")\n",
    "        print(group_name)\n",
    "        # load features\n",
    "        if group_name in file_features:\n",
    "            features = file_features[group_name][\"features\"]\n",
    "        else:\n",
    "            raise ValueError(group_name+\" not in \"+filename_features)\n",
    "        print(\"features\")\n",
    "        print(features)\n",
    "        # load returns\n",
    "        if group_name in file_returns:\n",
    "            returns = file_returns[group_name][\"returns\"]\n",
    "        else:\n",
    "            raise ValueError(group_name+\" not in \"+filename_features)\n",
    "        print(returns)\n",
    "        # load Symbols if calculate_roi is true\n",
    "        #TODO: Implement Symbol loading for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_in\n",
      "42343\n",
      "m_out\n",
      "42333\n",
      "samp_remaining\n",
      "42319\n",
      "batch\n",
      "42319\n",
      "returns.shape\n",
      "(42333, 4)\n",
      "nChannels+offset+2\n",
      "12\n",
      "nChannels+offset+batch+seq_len+2\n",
      "42334\n",
      "r_support.shape\n",
      "(42321,)\n",
      "nChannels+offset+batch+seq_len+2-(nChannels+offset+2)\n",
      "42322\n",
      "returns[-1,data.lookAheadIndex]\n",
      "9.999999999998899e-05\n",
      "r_support[-1]\n",
      "9.999999999998899e-05\n",
      "stds_out.shape\n",
      "(4,)\n",
      "totalSampsPerLevel\n",
      "[  320.  6215. 29138.  6239.   407.]\n",
      "m_t\n",
      "42319\n",
      "IO['pointer']\n",
      "42319\n"
     ]
    }
   ],
   "source": [
    "# Build IO from var\n",
    "\n",
    "# total number of possible channels\n",
    "nChannels = int(nExS/mW)\n",
    "# sequence length\n",
    "seq_len = model.seq_len#int((data.lB-data.nEventsPerStat)/data.movingWindow)\n",
    "# samples allocation per batch\n",
    "aloc = 2**20\n",
    "# extract means and stats\n",
    "means_in = stats['means_in']\n",
    "stds_in = stats['stds_in']\n",
    "m_in = stats['m_in']\n",
    "stds_out = stats['stds_out']\n",
    "m_out = stats['m_out']\n",
    "print(\"m_in\")\n",
    "print(m_in)\n",
    "print(\"m_out\")\n",
    "print(m_out)\n",
    "# add dateTimes, bids and asks if are included in file\n",
    "all_info = 0\n",
    "if calculate_roi:\n",
    "    all_info = 1\n",
    "    dts = symbols['DT']\n",
    "    bids = symbols['B']\n",
    "    asks = symbols['A']\n",
    "        \n",
    "    D = IO['D']\n",
    "    B = IO['B']\n",
    "    A = IO['A']\n",
    "\n",
    "# extract IO structures\n",
    "X = IO['X']\n",
    "Y = IO['Y']\n",
    "#I = IO['I']\n",
    "pointer = IO['pointer']\n",
    "\n",
    "feats_var_normed = np.minimum(np.maximum((features-means_in)/\\\n",
    "                     stds_in,-data.max_var),data.max_var)\n",
    "# get some scalars\n",
    "#nSamps = feats_var_normed.shape[0]\n",
    "samp_remaining = m_out-nChannels-seq_len-1\n",
    "print(\"samp_remaining\")\n",
    "print(samp_remaining)\n",
    "chunks = int(np.ceil(samp_remaining/aloc))\n",
    "# init counter of samps processed\n",
    "offset = 0\n",
    "# loop over chunks\n",
    "for i in range(chunks):\n",
    "    # this batch length\n",
    "    batch = np.min([samp_remaining,aloc])\n",
    "    print(\"batch\")\n",
    "    print(batch)\n",
    "    # create support numpy vectors to speed up iterations\n",
    "    v_support = feats_var_normed[offset:offset+batch+seq_len, :]\n",
    "    # get init and end indexes for returns\n",
    "    init_idx_rets = nChannels+offset+seq_len-1\n",
    "    end_idx_rets = nChannels+offset+batch+2*seq_len-1\n",
    "    r_support = returns[init_idx_rets:end_idx_rets, data.lookAheadIndex]\n",
    "    print(\"returns.shape\")\n",
    "    print(returns.shape)\n",
    "    print(\"nChannels+offset+2\")\n",
    "    print(nChannels+offset+2)\n",
    "    print(\"nChannels+offset+batch+seq_len+2\")\n",
    "    print(nChannels+offset+batch+seq_len+2)\n",
    "    print(\"r_support.shape\")\n",
    "    print(r_support.shape)\n",
    "    print(\"nChannels+offset+batch+seq_len+2-(nChannels+offset+2)\")\n",
    "    print(nChannels+offset+batch+seq_len+2-(nChannels+offset+2))\n",
    "    print(\"returns[-1,data.lookAheadIndex]\")\n",
    "    print(returns[-1,data.lookAheadIndex])\n",
    "    print(\"r_support[-1]\")\n",
    "    print(r_support[-1])\n",
    "    # we only take here the entry time index, and later at DTA building the \n",
    "    # exit time index is derived from the entry time and the number of events to\n",
    "    # look ahead\n",
    "    if calculate_roi:\n",
    "        dt_support = dts[init_idx_rets:end_idx_rets, [0,data.lookAheadIndex+1]]\n",
    "        b_support = bids[init_idx_rets:end_idx_rets, [0,data.lookAheadIndex+1]]\n",
    "        a_support = asks[init_idx_rets:end_idx_rets, [0,data.lookAheadIndex+1]]\n",
    "    # update remaining samps to proceed\n",
    "    samp_remaining = samp_remaining-batch\n",
    "    # init formatted input and output\n",
    "    X_i = np.zeros((batch, seq_len, features.shape[1]))\n",
    "    # real-valued output\n",
    "    O_i = np.zeros((batch, seq_len, 1))    \n",
    "    if calculate_roi:\n",
    "        # last dimension is to incorporate in and out symbols\n",
    "        D_i = np.chararray((batch, 2),itemsize=19)\n",
    "        B_i = np.zeros((batch, 2))\n",
    "        A_i = np.zeros((batch, 2))\n",
    "    \n",
    "    for nI in range(batch):\n",
    "        # get input\n",
    "        X_i[nI,:,:] = v_support[nI:nI+seq_len, :]            \n",
    "        # due to substraction of features for variation, output gets the \n",
    "        # feature one entry later\n",
    "        O_i[nI,:,0] = r_support[nI]\n",
    "        if calculate_roi:\n",
    "            D_i[nI,:] = dt_support[nI,:]\n",
    "            B_i[nI,:] = b_support[nI,:]\n",
    "            A_i[nI,:] = a_support[nI,:]\n",
    "    \n",
    "    # normalize output\n",
    "    print(\"stds_out.shape\")\n",
    "    print(stds_out.shape)\n",
    "    O_i = O_i/stds_out[data.lookAheadIndex]\n",
    "    # update counters\n",
    "    offset = offset+batch\n",
    "    # get decimal and binary outputs\n",
    "    Y_i, y_dec = _build_bin_output(model, O_i, batch)\n",
    "    # get samples per level\n",
    "    for l in range(model.size_output_layer):\n",
    "        totalSampsPerLevel[l] = totalSampsPerLevel[l]+np.sum(y_dec[:,-1,0]==l)\n",
    "    # resize IO structures\n",
    "    X.resize((pointer+batch, seq_len,features.shape[1]))\n",
    "    Y.resize((pointer+batch, seq_len,model.commonY+model.size_output_layer))\n",
    "    # update IO structures\n",
    "    X[pointer:pointer+batch,:,:] = X_i\n",
    "    Y[pointer:pointer+batch,:,:] = Y_i\n",
    "    if calculate_roi:\n",
    "        # resize\n",
    "        D.resize((pointer+batch, 2))\n",
    "        B.resize((pointer+batch, 2))\n",
    "        A.resize((pointer+batch, 2))\n",
    "        # update\n",
    "        D[pointer:pointer+batch,:] = D_i\n",
    "        B[pointer:pointer+batch,:] = B_i\n",
    "        A[pointer:pointer+batch,:] = A_i\n",
    "#        save_as_matfile('X_h_n_'+str(int(s/2)),'X_h_n'+str(int(s/2)),X_i)\n",
    "#        save_as_matfile('O_h_n_'+str(int(s/2)),'O_h_n'+str(int(s/2)),O_i)\n",
    "        \n",
    "    # uodate pointer\n",
    "    pointer += batch\n",
    "# end of for i in range(chunks):\n",
    "# update dictionary\n",
    "IO['X'] = X\n",
    "IO['Y'] = Y\n",
    "IO['pointer'] = pointer\n",
    "if calculate_roi:\n",
    "    IO['D'] = D\n",
    "    IO['B'] = B\n",
    "    IO['A'] = A\n",
    "print(\"totalSampsPerLevel\")\n",
    "print(totalSampsPerLevel)\n",
    "\n",
    "if if_build_IO:\n",
    "    ass_IO_ass[ass_idx] = IO['pointer']\n",
    "    #print(\"\\tTime for \"+thisAsset+\":\"+str(np.floor(time.time()-tic))+\"s\"+\n",
    "    #          \". Total time:\"+str(np.floor(time.time()-ticTotal))+\"s\")\n",
    "# end of assets loop?\n",
    "if if_build_IO:\n",
    "    f_IO.attrs.create('ass_IO_ass', ass_IO_ass, dtype=int)\n",
    "    f_IO.close()\n",
    "else:\n",
    "    # get ass_IO_ass from disk\n",
    "    f_IO = h5py.File(filename_IO,'r')\n",
    "    ass_IO_ass = f_IO.attrs.get(\"ass_IO_ass\")\n",
    "    f_IO.close()\n",
    "m_t = IO['pointer']\n",
    "print(\"m_t\")\n",
    "print(m_t)\n",
    "print(\"IO['pointer']\")\n",
    "print(IO['pointer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_IO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42319, 3, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
